{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "from spacy.tokens import Doc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "file_path = \"../data/indeed_data engineer_2021040519.txt\"\n",
    "job_data = []\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(',')\n",
    "        job_data.append(line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "job_data = pd.DataFrame(job_data[1:], columns=job_data[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        return Doc(self.vocab, words=words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_ngram(tokens, n):\n",
    "        ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "        return [\" \".join(gram) for gram in ngrams]\n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# stop_words = stopwords.words['english']\n",
    "user_define_stop_words = ['experience', 'work', 'job']\n",
    "\n",
    "def is_allowed_token(token):\n",
    "    return token.text.strip() != '' and not token.is_stop and token.lemma_ not in user_define_stop_words\n",
    "\n",
    "def generate_chunk_token(doc):\n",
    "    for noun_phrase in list(doc.noun_chunks):\n",
    "        noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_)\n",
    " \n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    clean and tokenize text\n",
    "    :param text: str\n",
    "    :return: a list of tokens\n",
    "    \"\"\"\n",
    "    # replace \\s with ' '\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    # replace punctuation with ' '\n",
    "    text = re.sub(f'[{string.punctuation}]+', ' ', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # really bad, some chunks are messy \n",
    "    # such as  data integration metadata management and business intelligence performance,\n",
    "    # generate_chunk_token(doc)\n",
    "    \n",
    "    tokens = [token for token in doc if is_allowed_token(token)]\n",
    "\n",
    "    return tokens\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "['data engineer',\n 'engineer schedule',\n 'schedule mon',\n 'mon fri',\n 'fri overview']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 69
    }
   ],
   "source": [
    "tokens = preprocess_text(\"\"\"\n",
    "Data Engineer,Schedule: Mon- Fri Overview: The data engineer structures data for analytical access performance and integration. The engineer must manage the balance of current and future needs in both design and content. This position designs and constructs enterprise-wide data and information architecture to support business intelligence and data integration management efforts. You will engineer and implement data warehouse and data mart concepts and design and oversee strategies for data integration metadata management and business intelligence performance. You will partner with the business and leadership to prioritize data and information requirements and will help determine strategic direction of data asset management efforts. Responsibilities: Works with analytics information manager to develop a practical roadmap for analytic resource(s) and reporting platform(s). Assist in defining the overall data architecture including ELT processes logical & physical data models and data mart designs.Partner with leadership to understand and prioritize data and information requirements and determine strategic direction for integration and data management of analytical data resources.Collaborate with key business users to identify needs and opportunities for improved delivery.Develops an understanding of the underlying corporate data sources and their relationship to support credit union data driven decision objectives.Maintain knowledge of software tools and programming languages that effectively support the environment.Lead continuous improvement efforts to enhance performance and provide increased functionality. Provide technical guidance to projects and teams ensuring that new initiatives enable effective analytic information management. Develop and oversee metadata repository business intelligence (BI) and data warehouse strategies and development.Engineer develop and implement data warehouse and data mart concepts.Provide technical knowledge and business support to BI teams.Lead data management efforts for enterprise projects through all phases of development life cycle.Be able to pilot and build POC’s to demonstrate and test business value Should have a strong understanding of relational and dimensional modeling to facilitate data wrangling code operationalization and exploration of data. Educate and provide data solutions to varied levels and use cases Develop presentations and trainings for business users to increase understanding of data resources and effective use casesContribute to and maintain business glossary and other resources that foster cross-functional collaboration on analytical data resources Cross-train support and work closely with team members in the following roles Data Scientist: data wrangling feature engineering model development management and operationalization. BI Analyst & Developer: visualizations report building analysis of data for consulting purposes and data mart development. Database Administration: database security redundancy and backup source system ETL Qualifications: QUALIFICATIONS: Must be able to manage multiple tasks simultaneously and react to problems quickly.Must have prior experience developing analytical solutions in large or midsize companies.Must be able to translate concepts and directions into practical solutions.Ability to handle confidential and sensitive data and meet critical deadlinesExcellent understanding of SQL PL/SQL and ELT implementation.Experience with multiple relational database platforms Oracle and SQL Server desired.Programming experience with 4th generation and 3rd generation computer languages such as Python and R desiredMust be able to develop maintain review and explain data models.Must have excellent verbal and written communication skills.Must be a team playerUnderstanding of the financial services industry desiredExperience with dashboard design and delivery desired EXPERIENCE: Four years of progressive work experience in business intelligence data warehousing analytics or data engineering.Experience working with end users to gather requirements and build technical solutions from concept to implementation. EDUCATION: Bachelor’s degree in information technology computer science or other business related field. Equivalent experience may substitute for education qualifications.\n",
    "\n",
    "\"\"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def calculate_token_df(token, docs):\n",
    "    \"\"\"\n",
    "    calculate the Document Frequency (occurance in all docs) of a token\n",
    "    :param token: \n",
    "    :param docs: [doc1, doc2, ..., doct], where doct is a set of tokens\n",
    "    :return: Document Frequency\n",
    "    \"\"\"\n",
    "    token_df = 0\n",
    "    for token_set in docs:\n",
    "        if token in token_set:\n",
    "            token_df += 1\n",
    "    return token_df\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "job_data[\"tokens\"] = job_data[\"job_description\"].apply(preprocess_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "docs = []\n",
    "all_unique_tokens = set()\n",
    "for tokens in job_data[\"tokens\"].tolist():\n",
    "    tokens = [token.text for token in tokens]\n",
    "    tokens = CustomTokenizer.generate_ngram(tokens, n=2)\n",
    "\n",
    "    token_set = set(tokens)\n",
    "    docs.append(token_set)\n",
    "    all_unique_tokens.update(token_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "df_dict = dict.fromkeys(all_unique_tokens, 0)\n",
    "for token in all_unique_tokens:\n",
    "    df_dict[token] = calculate_token_df(token, docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "[('5 years', 9),\n ('remote interview', 6),\n ('interview process', 6),\n ('data pipelines', 6),\n ('fully remote', 6),\n ('big data', 5),\n ('monday friday', 5),\n ('computer science', 5),\n ('data engineer', 5),\n ('bachelor degree', 5),\n ('bachelor s', 5),\n ('data sources', 4),\n ('paid time', 4),\n ('sql server', 4),\n ('data engineering', 4),\n ('knowledge data', 4),\n ('python r', 4),\n ('business intelligence', 4),\n ('data warehousing', 4),\n ('years required', 4)]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 91
    }
   ],
   "source": [
    "sorted(df_dict.items(), key=lambda item: -item[1])[:20]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}